{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNTSwaM6w8zK1OgMUqJniuD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4NYM46lrWYd0","executionInfo":{"status":"ok","timestamp":1708015209514,"user_tz":-420,"elapsed":23814,"user":{"displayName":"Hưởng Đỗ","userId":"14935951622527313821"}},"outputId":"f19534a3-c5ef-432f-dfd2-0765386a4ffc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!pip install pyspark"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NHVgnRRjWonY","executionInfo":{"status":"ok","timestamp":1708015264527,"user_tz":-420,"elapsed":55015,"user":{"displayName":"Hưởng Đỗ","userId":"14935951622527313821"}},"outputId":"ac679d36-d178-469f-94bb-6b6e45117d25"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pyspark\n","  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n","Building wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425345 sha256=edb2830286f142d4799ec02bae687e58f9ff994041c3b0aa7f353f829ccc71b1\n","  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n","Successfully built pyspark\n","Installing collected packages: pyspark\n","Successfully installed pyspark-3.5.0\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","\n","from pyspark.sql import SparkSession\n","from pyspark.sql import functions as f\n","from pyspark.sql.types import FloatType, IntegerType\n","from pyspark.sql.functions import col, when\n","from pyspark.sql.functions import udf\n","from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n","from pyspark.ml.feature import StopWordsRemover, RegexTokenizer\n","from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n","from pyspark.sql import Row\n","from pyspark.ml import Pipeline\n","from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier, RandomForestClassifier"],"metadata":{"id":"fSm0T4pvWrWx","executionInfo":{"status":"ok","timestamp":1708015265119,"user_tz":-420,"elapsed":601,"user":{"displayName":"Hưởng Đỗ","userId":"14935951622527313821"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["spark = SparkSession.builder.appName(\"Model\").config(\"spark.executor.memory\",\"4g\").getOrCreate()"],"metadata":{"id":"nx4EH8n8WtBc","executionInfo":{"status":"ok","timestamp":1708015278237,"user_tz":-420,"elapsed":13119,"user":{"displayName":"Hưởng Đỗ","userId":"14935951622527313821"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["import html\n","schema = \" free_text string, label_id int\"\n","spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n","\n","IN_PATH_RAW = \"/content/drive/MyDrive/yt_comment_sentiment/train.csv\"\n","IN_PATH_TEST = \"/content/drive/MyDrive/yt_comment_sentiment/test.csv\"\n","\n","spark_reader = spark.read.schema(schema)\n","\n","user_regex = r\"(@\\w{1,15})\"\n","hashtag_regex = \"(#\\w{1,})\"\n","url_regex=r\"((https?|ftp|file):\\/{2,3})+([-\\w+&@#/%=~|$?!:,.]*)|(www.)+([-\\w+&@#/%=~|$?!:,.]*)\"\n","email_regex=r\"[\\w.-]+@[\\w.-]+\\.[a-zA-Z]{1,}\"\n","\n","\n","@f.udf\n","def html_unescape(s: str):\n","    if isinstance(s, str):\n","        return html.unescape(s)\n","    return s\n","\n","\n","def clean_data(df):\n","    df = (\n","        df\n","        .withColumn(\"original_text\", f.col(\"free_text\"))\n","        # Remove numbers and characters from text\n","        .withColumn(\"free_text\",f.regexp_replace(f.col(\"free_text\"), \"[^a-zA-ZÀ-ỹà-ỹ']\", \" \",))\n","        .withColumn(\"free_text\", f.regexp_replace(f.col(\"free_text\"), \"'\", \"\"))\n","        # Remove white space\n","        .withColumn(\"free_text\",f.regexp_replace(f.col(\"free_text\"), \" +\", \" \"))\n","        .withColumn(\"free_text\",f.trim(f.col(\"free_text\")))\n","        # Lowercase\n","        .withColumn(\"free_text\",f.lower(f.col(\"free_text\")))\n","        .withColumn(\"free_text\", f.regexp_replace(f.col(\"free_text\"), url_regex, \"\"))\n","        .withColumn(\"free_text\", f.regexp_replace(f.col(\"free_text\"), email_regex, \"\"))\n","        .withColumn(\"free_text\", f.regexp_replace(f.col(\"free_text\"), user_regex, \"\"))\n","        .withColumn(\"free_text\", f.regexp_replace(f.col(\"free_text\"), \"#\", \" \"))\n","        .withColumn(\"free_text\", html_unescape(f.col(\"free_text\")))\n","        .filter(\"free_text != ''\")\n","    )\n","    return df\n","\n","df_train_raw = spark_reader.csv(IN_PATH_RAW)\n","df_train_clean = clean_data(df_train_raw)\n","df_train_clean = df_train_clean.na.drop()\n","df_test_raw = spark_reader.csv(IN_PATH_TEST)\n","df_test_clean = clean_data(df_test_raw)\n","df_test_clean = df_test_clean.na.drop()"],"metadata":{"id":"Z1Y1g8FRWvKY","executionInfo":{"status":"ok","timestamp":1708015286882,"user_tz":-420,"elapsed":8654,"user":{"displayName":"Hưởng Đỗ","userId":"14935951622527313821"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["%%time\n","from pyspark.ml.feature import (\n","    StopWordsRemover,\n","    Tokenizer,\n","    HashingTF,\n","    IDF,\n","    CountVectorizer,\n",")\n","from pyspark.sql.functions import udf\n","\n","with open('/content/drive/MyDrive/yt_comment_sentiment/Stopword.txt', 'r', encoding='utf-8') as file:\n","    vietnamese_stopwords = file.read().splitlines()\n","\n","tokenizer = Tokenizer(inputCol=\"free_text\", outputCol=\"words1\") # chuyển sang vector\n","vietnamese_stopwords_remover = StopWordsRemover(inputCol=\"words1\", outputCol=\"words2\", stopWords=vietnamese_stopwords)\n","hashing_tf = HashingTF(\n","    inputCol=\"words2\",\n","    outputCol=\"term_frequency\",\n","    numFeatures=20,\n",")\n","idf = IDF(\n","    inputCol=\"term_frequency\",\n","    outputCol=\"features\",\n","    minDocFreq=5,\n",")\n","(training_data, validation_data) = df_train_clean.randomSplit([0.8, 0.2], seed=42)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iP0Pfb34Xuqs","executionInfo":{"status":"ok","timestamp":1708015293132,"user_tz":-420,"elapsed":6252,"user":{"displayName":"Hưởng Đỗ","userId":"14935951622527313821"}},"outputId":"34fef012-45ff-49d6-e069-790ee1b27249"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["CPU times: user 73.4 ms, sys: 17.3 ms, total: 90.7 ms\n","Wall time: 6.32 s\n"]}]},{"cell_type":"code","source":["df1 = tokenizer.transform(training_data)\n","df2 = vietnamese_stopwords_remover.transform(df1) # xóa những từ trong stop-word\n","df3 = hashing_tf.transform(df2)\n","df4 = idf.fit(df3).transform(df3)\n","df4 = df4.na.drop()"],"metadata":{"id":"uof2MKOTX4MT","executionInfo":{"status":"ok","timestamp":1708015315861,"user_tz":-420,"elapsed":22738,"user":{"displayName":"Hưởng Đỗ","userId":"14935951622527313821"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["# **LogisticRegression**"],"metadata":{"id":"CMBkc9exYLp9"}},{"cell_type":"code","source":["lr = LogisticRegression(labelCol='label_id')\n","semantic_analysis_pipeline = Pipeline(\n","    stages=[tokenizer, vietnamese_stopwords_remover, hashing_tf, idf, lr]\n",")\n","semantic_analysis_model = semantic_analysis_pipeline.fit(training_data)\n","\n","trained_df = semantic_analysis_model.transform(training_data)\n","val_df = semantic_analysis_model.transform(validation_data)\n","test_df = semantic_analysis_model.transform(df_test_clean)\n","\n","from pyspark.ml.evaluation import RegressionEvaluator\n","from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n","\n","evaluator = MulticlassClassificationEvaluator(labelCol=\"label_id\", metricName=\"accuracy\")\n","\n","accuracy_val = evaluator.evaluate(val_df)\n","accuracy_test = evaluator.evaluate(test_df)\n","\n","print(\"\\nTesting Val Data:\")\n","print(f\"Accuracy: {accuracy_val*100:.5f}%\")\n","print(\"\\nTesting Data:\")\n","print(f\"Accuracy: {accuracy_test*100:.5f}%\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sM3orc12X5D_","executionInfo":{"status":"ok","timestamp":1708015348631,"user_tz":-420,"elapsed":32780,"user":{"displayName":"Hưởng Đỗ","userId":"14935951622527313821"}},"outputId":"27f9baf5-a2ae-414f-de9c-4c22155d6f4a"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Testing Val Data:\n","Accuracy: 83.12566%\n","\n","Testing Data:\n","Accuracy: 82.69318%\n"]}]},{"cell_type":"markdown","source":["# **DecisionTreeClassifier**"],"metadata":{"id":"Ch-FFgZFYaeb"}},{"cell_type":"code","source":["from pyspark.ml.evaluation import RegressionEvaluator\n","from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n","\n","dt = DecisionTreeClassifier(labelCol='label_id')\n","semantic_analysis_pipeline = Pipeline(\n","    stages=[tokenizer, vietnamese_stopwords_remover, hashing_tf, idf, dt]\n",")\n","semantic_analysis_model = semantic_analysis_pipeline.fit(training_data)\n","\n","trained_df = semantic_analysis_model.transform(training_data)\n","val_df = semantic_analysis_model.transform(validation_data)\n","test_df = semantic_analysis_model.transform(df_test_clean)\n","\n","evaluator = MulticlassClassificationEvaluator(labelCol=\"label_id\", metricName=\"accuracy\")\n","\n","accuracy_val = evaluator.evaluate(val_df)\n","accuracy_test = evaluator.evaluate(test_df)\n","\n","print(\"\\nTesting Val Data:\")\n","print(f\"Accuracy: {accuracy_val*100:.5f}%\")\n","print(\"\\nTesting Data:\")\n","print(f\"Accuracy: {accuracy_test*100:.5f}%\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7Uh_y25kYHKi","executionInfo":{"status":"ok","timestamp":1708015378768,"user_tz":-420,"elapsed":30147,"user":{"displayName":"Hưởng Đỗ","userId":"14935951622527313821"}},"outputId":"c03c394e-3577-4fb2-ce98-b6e68c8df9dc"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Testing Val Data:\n","Accuracy: 82.57656%\n","\n","Testing Data:\n","Accuracy: 82.43510%\n"]}]},{"cell_type":"markdown","source":["# **RandomForestClassifier**"],"metadata":{"id":"fh9Fjql_YPPF"}},{"cell_type":"code","source":["rf = RandomForestClassifier(labelCol='label_id')\n","from pyspark.ml.evaluation import RegressionEvaluator\n","from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n","\n","dt = DecisionTreeClassifier(labelCol='label_id')\n","semantic_analysis_pipeline = Pipeline(\n","    stages=[tokenizer, vietnamese_stopwords_remover, hashing_tf, idf, rf]\n",")\n","semantic_analysis_model = semantic_analysis_pipeline.fit(training_data)\n","\n","trained_df = semantic_analysis_model.transform(training_data)\n","val_df = semantic_analysis_model.transform(validation_data)\n","test_df = semantic_analysis_model.transform(df_test_clean)\n","\n","evaluator = MulticlassClassificationEvaluator(labelCol=\"label_id\", metricName=\"accuracy\")\n","\n","accuracy_val = evaluator.evaluate(val_df)\n","accuracy_test = evaluator.evaluate(test_df)\n","\n","print(\"\\nTesting Val Data:\")\n","print(f\"Accuracy: {accuracy_val*100:.5f}%\")\n","print(\"\\nTesting Data:\")\n","print(f\"Accuracy: {accuracy_test*100:.5f}%\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"70mz0rLvYORI","executionInfo":{"status":"ok","timestamp":1708015411062,"user_tz":-420,"elapsed":32306,"user":{"displayName":"Hưởng Đỗ","userId":"14935951622527313821"}},"outputId":"be6a603e-82a5-4ea4-f3a7-111bc9acf752"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Testing Val Data:\n","Accuracy: 82.87223%\n","\n","Testing Data:\n","Accuracy: 82.82982%\n"]}]}]}